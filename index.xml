<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pranay Modukuru</title>
    <link>https://pranaymodukuru.github.io/</link>
      <atom:link href="https://pranaymodukuru.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Pranay Modukuru</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Pranay Modukuru 2020</copyright><lastBuildDate>Mon, 09 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Pranay Modukuru</title>
      <link>https://pranaymodukuru.github.io/</link>
    </image>
    
    <item>
      <title>Concrete Compressive Strength Prediction</title>
      <link>https://pranaymodukuru.github.io/project/concrete-compressive-strength/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://pranaymodukuru.github.io/project/concrete-compressive-strength/</guid>
      <description>&lt;p&gt;Photo by Ricardo 
&lt;a href=&#34;https://unsplash.com/@ripato&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gomez Angel&lt;/a&gt; on 
&lt;a href=&#34;https://unsplash.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unsplash&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;concrete-compressive-strength-prediction-using-machine-learning&#34;&gt;Concrete Compressive Strength Prediction using Machine Learning&lt;/h2&gt;
&lt;p&gt;Concrete is one of the most important materials in Civil Engineering. Knowing the compressive strength of concrete is very important when constructing a building or a bridge. The Compressive Strength of Concrete is a highly nonlinear function of ingredients used in making it and their characteristics. Thus, using Machine Learning to predict the Strength could be useful in generating a combination of ingredients which result in high Strength.&lt;/p&gt;
&lt;p&gt;Please read this 
&lt;a href=&#34;https://towardsdatascience.com/concrete-compressive-strength-prediction-using-machine-learning-4a531b3c43f3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt; for more explanation. Also leave some claps to show appreciation!&lt;/p&gt;
&lt;p&gt;Please refer 
&lt;a href=&#34;https://github.com/pranaymodukuru/Concrete-compressive-strength/blob/master/ConcreteCompressiveStrengthPrediction.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;ConcreteCompressiveStrengthPrediction.ipynb&lt;/em&gt;&lt;/a&gt; for code.&lt;/p&gt;
&lt;h2 id=&#34;1-problem-statement&#34;&gt;1. Problem Statement&lt;/h2&gt;
&lt;p&gt;Predicting Compressive Strength of Concrete given its age and quantitative measurements of ingredients.&lt;/p&gt;
&lt;h2 id=&#34;2-data-description&#34;&gt;2. Data Description&lt;/h2&gt;
&lt;p&gt;Data is obtained from UCI Machine Learning Repository.
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of instances - 1030&lt;/li&gt;
&lt;li&gt;Number of Attributes - 9
&lt;ul&gt;
&lt;li&gt;Attribute breakdown - 8 quantitative inputs, 1 quantitative output&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;attribute-information&#34;&gt;Attribute information&lt;/h4&gt;
&lt;h5 id=&#34;inputs&#34;&gt;Inputs&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Cement&lt;/li&gt;
&lt;li&gt;Blast Furnace Slag&lt;/li&gt;
&lt;li&gt;Fly Ash&lt;/li&gt;
&lt;li&gt;Water&lt;/li&gt;
&lt;li&gt;Superplasticizer&lt;/li&gt;
&lt;li&gt;Coarse Aggregate&lt;/li&gt;
&lt;li&gt;Fine Aggregate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All above features measured in kg/$m^3$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age (in days)&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;output&#34;&gt;Output&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Concrete Compressive Strength (Mpa)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-modelling-and-evaluation&#34;&gt;3. Modelling and Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Algorithms used&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression&lt;/li&gt;
&lt;li&gt;Lasso regression&lt;/li&gt;
&lt;li&gt;Ridge regression&lt;/li&gt;
&lt;li&gt;Decision Trees&lt;/li&gt;
&lt;li&gt;Random Forests&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Metric - Since the target variable is a continuous variable, regression evaluation metric RMSE (Root Mean Squared Error) and R2 Score (Coefficient of Determination) have been used.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-results&#34;&gt;4. Results&lt;/h2&gt;
&lt;h4 id=&#34;feature-correlation&#34;&gt;Feature correlation&lt;/h4&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/pearson_coeff.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/pearson_coeff.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h4 id=&#34;feature-importance&#34;&gt;Feature importance&lt;/h4&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/tree_feat_imps.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/tree_feat_imps.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h4 id=&#34;final-comparison&#34;&gt;Final Comparison&lt;/h4&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/final.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/final.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;5-references&#34;&gt;5. References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Concrete Compressive Strength Prediction using Machine Learning</title>
      <link>https://pranaymodukuru.github.io/post/concrete-compressive-strength/</link>
      <pubDate>Thu, 05 Mar 2020 01:53:52 +0100</pubDate>
      <guid>https://pranaymodukuru.github.io/post/concrete-compressive-strength/</guid>
      <description>&lt;p&gt;Photo by Ricardo 
&lt;a href=&#34;https://unsplash.com/@ripato&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gomez Angel&lt;/a&gt; on 
&lt;a href=&#34;https://unsplash.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unsplash&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;concrete-compressive-strength&#34;&gt;Concrete Compressive Strength&lt;/h2&gt;
&lt;p&gt;The Compressive Strength of Concrete determines the quality of Concrete. This is generally determined by a standard crushing test on a concrete cylinder. This requires engineers to build small concrete cylinders with different combinations of raw materials and test these cylinders for strength variations with a change in each raw material. The recommended wait time for testing the cylinder is 28 days to ensure correct results. This consumes a lot of time and requires lot of labour to prepare different prototypes and test them. Also, this method is prone to human error and one small mistake can cause the wait time to drastically increase.&lt;/p&gt;
&lt;p&gt;One way of reducing the wait time and reducing the amount of combinations to try is to make use of digital simulations, where we can provide information to the computer about what we know and the computer tries different combinations to predict the compressive strength. This way we can reduce the amount of combinations we can try physically and reduce the amount of time for experimentation. But, to design such software we have to know the relations between all the raw materials and how one material affects the strength. It is possible to derive mathematical equations and run simulations based on these equations, but we cannot expect the relations to be same in real-world. Also, these tests have been performed for many number of time now and we have enough real-world data that can be used for predictive modelling.&lt;/p&gt;
&lt;p&gt;In this article, we are going to analyse 
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Concrete&amp;#43;Compressive&amp;#43;Strength&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Concrete Compressive Strength&lt;/a&gt; dataset and build Machine Learning models to predict the compressive strength. This 
&lt;a href=&#34;https://github.com/pranaymodukuru/Concrete-compressive-strength/blob/master/ConcreteCompressiveStrengthPrediction.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt; containing all the code can be used in parallel.&lt;/p&gt;
&lt;h3 id=&#34;dataset-description&#34;&gt;Dataset Description&lt;/h3&gt;
&lt;p&gt;The dataset consists of 1030 instances with 9 attributes and has no missing values. There are 8 input variables and 1 output variable. Seven input variables represents the amount of a raw material (measured in $kg/m^3$) and one represents Age (in Days). The target variable is Concrete Compressive Strength measured in ($MPa$ - Mega Pascal). We shall explore the data to see how input features are affecting compressive strength.&lt;/p&gt;
&lt;h3 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h3&gt;
&lt;p&gt;The first step in a Data Science project is to understand the data and gain insights from the data before doing any modelling. This includes checking for any missing values, plotting the features with respect to the target variable, observing the distributions of all the features and so on. Lets import the data and
start analysing.&lt;/p&gt;
&lt;p&gt;Lets check the correlations between the input features, this will give an idea about how each variable is affecting all other variables. This can be done by calculating Pearson correlations between the features as shown in the code below.&lt;/p&gt;
&lt;!-- **Note** - Complete code used for generating plots (titles, axes labels, etc.) is not shown here for simplicity. The complete code can be viewed [here](https://github.com/pranaymodukuru/Concrete-compressive-strength/blob/master/ConcreteCompressiveStrengthPrediction.ipynb). --&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;corr = data.corr()
sns.heatmap(corr, annot=True, cmap=&#39;Blues&#39;)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/pearson_coeff.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/pearson_coeff.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We can observe a high positive correlation between &lt;strong&gt;compressive Strength&lt;/strong&gt; (CC_Strength) and &lt;strong&gt;Cement&lt;/strong&gt;. this is true because strength concrete indeed increases with an increase in amount of cement used in preparing it. Also, &lt;strong&gt;Age&lt;/strong&gt; and &lt;strong&gt;Super Plasticizer&lt;/strong&gt; are other two factors influencing Compressive strength.&lt;/p&gt;
&lt;p&gt;There are other strong correlations between the fetures,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A strong negative correlation between &lt;strong&gt;Super Plasticizer&lt;/strong&gt; and &lt;strong&gt;Water&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;positive correlations between &lt;strong&gt;Super Plasticizer&lt;/strong&gt; and &lt;strong&gt;Fly Ash&lt;/strong&gt;, &lt;strong&gt;Fine Aggregate&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These correlations are useful to understand the data in detail, as they give an idea about how a variable is affecting the other. We can further use a &lt;strong&gt;pairplot&lt;/strong&gt; in seaborn to plot pair wise relations between all the features and distributions of features along the diagonal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.pairplot(data)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/pairplot.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/pairplot.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The pair plot gives a visual representation of correlations between all the features.&lt;/p&gt;
&lt;p&gt;We can plot scatter plots between &lt;strong&gt;CC_Strength&lt;/strong&gt; and other features to see more complex relations.&lt;/p&gt;
&lt;h5 id=&#34;cc_strength-vs-cement-age-water&#34;&gt;CC_Strength vs (Cement, Age, Water)&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(y=&amp;quot;CC_Strength&amp;quot;, x=&amp;quot;Cement&amp;quot;, hue=&amp;quot;Water&amp;quot;,
                  size=&amp;quot;Age&amp;quot;, data=data, ax=ax, sizes=(50, 300))
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/scatter_1.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/scatter_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The observations we can make from this plot,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Compressive strength increases as amount of cement increases&lt;/strong&gt;, as the dots move up when we move towards right on the x-axis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compressive strength increases with age&lt;/strong&gt; (as the size of dots represents the age), this not the case always but can be up to an extent.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cement with less age requires more cement for higher strength&lt;/strong&gt;, as the smaller dots are moving up when we move towards right on x-axis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The older the cement is the more water it requires&lt;/strong&gt;, can be confirmed by observing the colour of the dots. Larger dots with dark colour indicate high age and more water.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete strength increases when less water is used&lt;/strong&gt; in preparing it, since the dots on the lower side (y-axis) are darker and the dots on higher end (y-axis) are brighter.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;cc-strength-vs-fine-aggregate-super-plasticizer-fly-ash&#34;&gt;CC Strength vs (Fine aggregate, Super Plasticizer, Fly Ash)&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.scatterplot(y=&amp;quot;CC_Strength&amp;quot;, x=&amp;quot;FineAggregate&amp;quot;, hue=&amp;quot;FlyAsh&amp;quot;, size=&amp;quot;Superplasticizer&amp;quot;,
                data=data, ax=ax, sizes=(50, 300))
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/scatter_2.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/scatter_2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Observations,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Compressive strength decreases Fly ash increases&lt;/strong&gt;, as more darker dots are concentrated in the region representing low compressive strength.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compressive strength increases with Super plasticizer&lt;/strong&gt;, since larger the dot the higher they are in the plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can visually understand 2D, 3D and max up to 4D plots (features represented by colour and size) as shown above, we can further use row wise and column wise plotting features by seaborn to do further analysis, but still we lack the ability to track all these correlations by ourselves. For this reason, we can turn to Machine Learning to capture these relations and give better insights into the problem.&lt;/p&gt;
&lt;h3 id=&#34;data-preprocessing&#34;&gt;Data preprocessing&lt;/h3&gt;
&lt;p&gt;Before we fit machine learning models on the data, we need to split the data into train, test splits. The features can be rescaled to have a mean of zero and a standard deviation of 1 i.e. all the features fall into the same range.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = data.iloc[:,:-1]         # Features
y = data.iloc[:,-1]          # Target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;model-building&#34;&gt;Model Building&lt;/h3&gt;
&lt;p&gt;After preparing the data, we can fit different models on the training data and compare their performance to choose the algorithm with good performance. As this is a regression problem, we can use RMSE (Root Mean Square Error) and $R^2$ score as evaluation metrics.&lt;/p&gt;
&lt;h4 id=&#34;1-linear-regression&#34;&gt;1. Linear Regression&lt;/h4&gt;
&lt;p&gt;We will start with Linear Regression, since this is the go-to algorithm for any regression problem. The algorithm tries to form a linear relationship between the input features and the target variable i.e. it fits a straight line given by, $$y = W*X + b = \sum_{i=1}^{n} w_i * x_i + b$$ Where $w_i$ corresponds to the coefficient of feature $x_i$.&lt;/p&gt;
&lt;p&gt;The magnitude of these coefficients can be further controlled by using regularization terms to the cost functions. Adding the sum of the magnitudes of the coefficients will result in the coefficients being close to zero, this variation of linear regression is called &lt;strong&gt;Lasso&lt;/strong&gt; Regression. Adding the sum of squares of the coefficients to the cost function will make the coefficients be in the same range and this variation is called &lt;strong&gt;Ridge&lt;/strong&gt; Regression. Both these variations help in reducing the model complexity and therefore reducing the chances of overfitting on the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Importing models
from sklearn.linear_model import LinearRegression, Lasso, Ridge

# Linear Regression
lr = LinearRegression()
# Lasso Regression
lasso = Lasso()
# Ridge Regression
ridge = Ridge()

# Fitting models on Training data
lr.fit(X_train, y_train)
lasso.fit(X_train, y_train)
ridge.fit(X_train, y_train)

# Making predictions on Test data
y_pred_lr = lr.predict(X_test)
y_pred_lasso = lasso.predict(X_test)
y_pred_ridge = ridge.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print(&amp;quot;Model\t\t\t RMSE \t\t R2&amp;quot;)
print(&amp;quot;&amp;quot;&amp;quot;LinearRegression \t {:.2f} \t\t{:.2f}&amp;quot;&amp;quot;&amp;quot;.format(
            np.sqrt(mean_squared_error(y_test, y_pred_lr)), r2_score(y_test, y_pred_lr)))
print(&amp;quot;&amp;quot;&amp;quot;LassoRegression \t {:.2f} \t\t{:.2f}&amp;quot;&amp;quot;&amp;quot;.format(
            np.sqrt(mean_squared_error(y_test, y_pred_lasso)), r2_score(y_test, y_pred_lasso)))
print(&amp;quot;&amp;quot;&amp;quot;RidgeRegression \t {:.2f} \t\t{:.2f}&amp;quot;&amp;quot;&amp;quot;.format(
            np.sqrt(mean_squared_error(y_test, y_pred_ridge)), r2_score(y_test, y_pred_ridge)))
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;output&#34;&gt;Output&lt;/h6&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;th&gt;R2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LinearRegression&lt;/td&gt;
&lt;td&gt;10.29&lt;/td&gt;
&lt;td&gt;0.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LassoRegression&lt;/td&gt;
&lt;td&gt;10.68&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RidgeRegression&lt;/td&gt;
&lt;td&gt;10.29&lt;/td&gt;
&lt;td&gt;0.57&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There is not much difference between the performance with these three algorithms, we can plot the coefficients assigned by the three algorithms for the features with the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;coeff_lr = lr.coef_
coeff_lasso = lasso.coef_
coeff_ridge = ridge.coef_

labels = req_col_names[:-1]

x = np.arange(len(labels))
width = 0.3

fig, ax = plt.subplots(figsize=(10,6))
rects1 = ax.bar(x - 2*(width/2), coeff_lr, width, label=&#39;LR&#39;)
rects2 = ax.bar(x, coeff_lasso, width, label=&#39;Lasso&#39;)
rects3 = ax.bar(x + 2*(width/2), coeff_ridge, width, label=&#39;Ridge&#39;)

ax.set_ylabel(&#39;Coefficient&#39;)
ax.set_xlabel(&#39;Features&#39;)
ax.set_title(&#39;Feature Coefficients&#39;)
ax.set_xticks(x)
ax.set_xticklabels(labels, rotation=45)
ax.legend()

def autolabel(rects):
    &amp;quot;&amp;quot;&amp;quot;Attach a text label above each bar in *rects*, displaying its height.&amp;quot;&amp;quot;&amp;quot;
    for rect in rects:
        height = rect.get_height()
        ax.annotate(&#39;{:.2f}&#39;.format(height), xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3), textcoords=&amp;quot;offset points&amp;quot;, ha=&#39;center&#39;, va=&#39;bottom&#39;)
autolabel(rects1)
autolabel(rects2)
autolabel(rects3)

fig.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/lr_coeffs.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/lr_coeffs.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;As seen in the figure, Lasso regression pushes the coefficients towards zero and the coefficients with the normal Linear Regression and Ridge Regression are almost the same.&lt;/p&gt;
&lt;p&gt;We can further see how the predictions are by plotting the true values and predicted values,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,4))

ax1.scatter(y_pred_lr, y_test, s=20)
ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &#39;k--&#39;, lw=2)
ax1.set_ylabel(&amp;quot;True&amp;quot;)
ax1.set_xlabel(&amp;quot;Predicted&amp;quot;)
ax1.set_title(&amp;quot;Linear Regression&amp;quot;)

ax2.scatter(y_pred_lasso, y_test, s=20)
ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &#39;k--&#39;, lw=2)
ax2.set_ylabel(&amp;quot;True&amp;quot;)
ax2.set_xlabel(&amp;quot;Predicted&amp;quot;)
ax2.set_title(&amp;quot;Lasso Regression&amp;quot;)

ax3.scatter(y_pred_ridge, y_test, s=20)
ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &#39;k--&#39;, lw=2)
ax3.set_ylabel(&amp;quot;True&amp;quot;)
ax3.set_xlabel(&amp;quot;Predicted&amp;quot;)
ax3.set_title(&amp;quot;Ridge Regression&amp;quot;)

fig.suptitle(&amp;quot;True vs Predicted&amp;quot;)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/lr_true_pred.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/lr_true_pred.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;If the predicted values and the target values are equal, then the points on the scatter plot will lie on the straight line. As we can see here, non of the model predicts the Compressive Strength correctly.&lt;/p&gt;
&lt;h4 id=&#34;2-decision-trees&#34;&gt;2. Decision Trees&lt;/h4&gt;
&lt;p&gt;A Decision Tree Algorithm represents the data with a tree like structure, where each node represents a decision taken on a feature. This algorithm would give better performance in this case, since we have a lot of zeros in some of the input features as seen from their distributions in the pair plot above. This would help the decision trees build trees based on some conditions on features which can further improve performance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeRegressor

dtr = DecisionTreeRegressor()

dtr.fit(X_train, y_train)

y_pred_dtr = dtr.predict(X_test)

print(&amp;quot;Model\t\t\t\t RMSE  \t\t R2&amp;quot;)
print(&amp;quot;&amp;quot;&amp;quot;Decision Tree Regressor \t {:.2f} \t\t{:.2f}&amp;quot;&amp;quot;&amp;quot;.format(
            np.sqrt(mean_squared_error(y_test, y_pred_dtr)), r2_score(y_test, y_pred_dtr)))

plt.scatter(y_test, y_pred_dtr)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &#39;k--&#39;, lw=2)
plt.xlabel(&amp;quot;Predicted&amp;quot;)
plt.ylabel(&amp;quot;True&amp;quot;)
plt.title(&amp;quot;Decision Tree Regressor&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;th&gt;R2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Decision Tree Regressor&lt;/td&gt;
&lt;td&gt;7.31&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/dtr_true_pred.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/dtr_true_pred.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The Root Mean Squared Error (RMSE) has come down from 10.29 to 7.31, so the Decision Tree Regressor has improved the performance by a significant amount. This can be observed in the plot as well as more points are closer to the line.&lt;/p&gt;
&lt;h4 id=&#34;3-random-forests&#34;&gt;3. Random Forests&lt;/h4&gt;
&lt;p&gt;Since Using a Decision Tree Regressor has improved our performance, we can further improve the performance by ensembling more trees. Random Forest Regressor trains randomly initialized trees with random subsets of data sampled from the training data, this will make our model more robust.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor(n_estimators=100)

rfr.fit(X_train, y_train)

y_pred_rfr = rfr.predict(X_test)

print(&amp;quot;Model\t\t\t\t RMSE  \t\t R2&amp;quot;)
print(&amp;quot;&amp;quot;&amp;quot;Random Forest Regressor \t {:.2f} \t\t{:.2f}&amp;quot;&amp;quot;&amp;quot;.format(
            np.sqrt(mean_squared_error(y_test, y_pred_rfr)), r2_score(y_test, y_pred_rfr)))

plt.scatter(y_test, y_pred_rfr)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &#39;k--&#39;, lw=2)
plt.xlabel(&amp;quot;Predicted&amp;quot;)
plt.ylabel(&amp;quot;True&amp;quot;)
plt.title(&amp;quot;Random Forest Regressor&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;th&gt;R2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest Regressor&lt;/td&gt;
&lt;td&gt;5.08&lt;/td&gt;
&lt;td&gt;0.89&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/rfr_true_pred.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/rfr_true_pred.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The RMSE has further reduced by ensembling multiple trees. We can plot the feature importance&amp;rsquo;s for tree based models. The feature importance&amp;rsquo;s show how important a feature is for a model when making a prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
feature_dtr = dtr.feature_importances_
feature_rfr = rfr.feature_importances_

labels = req_col_names[:-1]

x = np.arange(len(labels))
width = 0.3

fig, ax = plt.subplots(figsize=(10,6))
rects1 = ax.bar(x-(width/2), feature_dtr, width, label=&#39;Decision Tree&#39;)
rects2 = ax.bar(x+(width/2), feature_rfr, width, label=&#39;Random Forest&#39;)

ax.set_ylabel(&#39;Importance&#39;)
ax.set_xlabel(&#39;Features&#39;)
ax.set_title(&#39;Feature Importance&#39;)
ax.set_xticks(x)
ax.set_xticklabels(labels, rotation=45)
ax.legend(loc=&amp;quot;upper left&amp;quot;, bbox_to_anchor=(1,1))

autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/tree_feat_imps.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/tree_feat_imps.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Cement and Age are treated as the most important features by tree based models. Fly ash, Coarse and Fine aggregates are least important factors when predicting the strength of Concrete.&lt;/p&gt;
&lt;h4 id=&#34;comparison&#34;&gt;Comparison&lt;/h4&gt;
&lt;p&gt;Finally, lets compare the results of all the algorithms.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
models = [lr, lasso, ridge, dtr, rfr]
names = [&amp;quot;Linear Regression&amp;quot;, &amp;quot;Lasso Regression&amp;quot;, &amp;quot;Ridge Regression&amp;quot;,
         &amp;quot;Decision Tree Regressor&amp;quot;, &amp;quot;Random Forest Regressor&amp;quot;]
rmses = []

for model in models:
    rmses.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))

x = np.arange(len(names))
width = 0.3

fig, ax = plt.subplots(figsize=(10,7))
rects = ax.bar(x, rmses, width)
ax.set_ylabel(&#39;RMSE&#39;)
ax.set_xlabel(&#39;Models&#39;)
ax.set_title(&#39;RMSE with Different Algorithms&#39;)
ax.set_xticks(x)
ax.set_xticklabels(names, rotation=45)
autolabel(rects)
fig.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/final.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/concrete-compressive-strength-imgs/final.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We have analysed the Compressive Strength Data and used Machine Learning to predict the Compressive Strength of Concrete. We have used Linear Regression and its variations, Decision Trees and Random Forests to make predictions and compared their performance. Random Forest Regressor has the lowest RMSE and is a good choice for this problem. Also, we can further improve the performance of the algorithm by tuning the hyperparameters by performing a grid search or random search.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;I-Cheng Yeh, &amp;ldquo;
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0008884698001653&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modeling of strength of high performance concrete using artificial neural networks&lt;/a&gt;,&amp;rdquo; Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998).&lt;/li&gt;
&lt;li&gt;Ahsanul Kabir, Md Monjurul Hasan, Khasro Miah, &amp;ldquo;
&lt;a href=&#34;https://www.researchgate.net/publication/258255660_Strength_Prediction_Model_for_Concrete&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Strength Prediction Model for Concrete&lt;/a&gt;&amp;quot;, ACEE Int. J. on Civil and Environmental Engineering, Vol. 2, No. 1, Aug 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Effect of Initialization on Optimization Trajectory in 2D</title>
      <link>https://pranaymodukuru.github.io/post/optimization-trajectory/</link>
      <pubDate>Thu, 05 Mar 2020 01:53:52 +0100</pubDate>
      <guid>https://pranaymodukuru.github.io/post/optimization-trajectory/</guid>
      <description>&lt;h3 id=&#34;the-optimization-problem&#34;&gt;The Optimization Problem&lt;/h3&gt;
&lt;p&gt;In simple words, finding a minimum value for a given equation is considered as optimization. This has many applications in real life - finding the fastest path when traveling from one place to other, job shop scheduling, air traffic management etc,. The optimization has been the back bone of machine learning, where the algorithms are expected to extract knowledge from huge volumes of data.&lt;/p&gt;
&lt;p&gt;Optimization plays a major role Neural Networks where there are millions of parameters and the goal is to find the right set of parameters to correctly represent the data. There has been a lot of research in this field and many algorithms have been developed for effective optimization. Even though the performance of the optimizer has improved a lot, there is another problem that the optimization depends upon i.e. the initial point. The trajectory of optimization is largely dependant on the initialisation. This has been studied and numerous initialization techniques have been proposed to effectively exploit the power of optimization algorithms.&lt;/p&gt;
&lt;p&gt;In this post we are going to see how the initialization can affect the performance of some of the optimization algorithms until day. Although, we are using a two dimensional problem here since it is easy to visualize, the initialization problem becomes more prevalent when there are millions of parameters (Neural Networks).&lt;/p&gt;
&lt;h3 id=&#34;the-task&#34;&gt;The Task&lt;/h3&gt;
&lt;p&gt;Initialize x, y and use gradient descent algorithms to find the optimal values of x and y such that the value of the Beale function is zero (or as low as possible).&lt;/p&gt;
&lt;h3 id=&#34;a-brief-introduction-to-optimization-algorithms&#34;&gt;A brief introduction to optimization algorithms&lt;/h3&gt;
&lt;p&gt;We are going to consider three popular optimization algorithms, since we are more concerned about the initialization these will be sufficient for our analysis.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt; - The stochastic gradient descent (SGD) algorithm performs one update at a time computing gradients at each step.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Momentum&lt;/strong&gt; - Overcomes the difficulty of slow updates of stochastic gradient descent by considering the momentum of gradients over a period of time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adam&lt;/strong&gt; - Considered to be the most popular optimization algorithm. It takes into consideration the first and second moments i.e. the exponentially decaying average of past gradients and squared gradients.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A more detailed explanation about gradient descent optimization algorithms, please read this 
&lt;a href=&#34;https://ruder.io/optimizing-gradient-descent/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; by 
&lt;a href=&#34;https://ruder.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Ruder&lt;/a&gt;.&lt;/p&gt;
&lt;h6 id=&#34;importing-required-libraries&#34;&gt;Importing required libraries&lt;/h6&gt;
&lt;p&gt;We are going to use the autograd functionality of 
&lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch&lt;/a&gt; for getting the gradients and matplotlib for plotting the trajectories.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import LogNorm

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-beale-function&#34;&gt;The Beale Function&lt;/h3&gt;
&lt;p&gt;$$
f(x, y) =  (1.5 - x + xy)^2 + (2.25 - x + xy^2)^2 +(2.625 - x + xy^3)^2\tag{1}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beale function is a multimodal non-convex continuous function defined in two dimensions.&lt;/li&gt;
&lt;li&gt;It is usually evaluated in the range $(x, y) \in [-4.5, 4.5]$.&lt;/li&gt;
&lt;li&gt;The function has only one global minimum at $(x, y) = (3, 0.5)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://www.sfu.ca/~ssurjano/beale.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beale&lt;/a&gt; Function, a two dimentional function is chosen to make visualizations simple.&lt;/p&gt;
&lt;h6 id=&#34;visualizing-the-beale-function&#34;&gt;Visualizing the Beale Function&lt;/h6&gt;
&lt;p&gt;As the Beale function is a two variable function ranging between -4.5 and 4.5, we can generate a meshgrid using numpy to pass all the possible values of x and y to the function. This enables us to have the output of the beale&amp;rsquo;s function at each possible point, we can use these outputs to visualize the function in a contour plot.&lt;/p&gt;
&lt;p&gt;As we are relating the optimization problem with neural networks, we will refer to &lt;strong&gt;(x, y)&lt;/strong&gt; as &lt;strong&gt;(w1 , w2)&lt;/strong&gt;. Also, when using a neural network we refer to objective function as a loss function and the output of the function as loss. In this case, we refer to the Beale&amp;rsquo;s function as &lt;strong&gt;loss function&lt;/strong&gt; and the outputs as &lt;strong&gt;losses&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Defining function
f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2

# Defining the range of w1 and w2, step size
w1_min, w1_max, w1_step = -4.5, 4.5, .2
w2_min, w2_max, w2_step = -4.5, 4.5, .2

# Global minima of the function
minima_ = [3, 0.5]

# generating meshgrid
w1, w2 = np.meshgrid(np.arange(w1_min, w1_max + w1_step, w1_step),
                     np.arange(w2_min, w2_max + w2_step, w2_step))
losses = f(w1, w2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now plot the losses on a contour plot with the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(10, 6))
ax.contour(w1, w2, losses, levels=np.logspace(0, 5, 35),
                    norm=LogNorm(), cmap=plt.cm.jet, alpha = 0.8)
ax.plot(*minima_, &#39;r*&#39;, color=&#39;r&#39;,
                    markersize=10, alpha=0.7, label=&#39;minima&#39;)
ax.set_xlabel(&#39;w1&#39;)
ax.set_ylabel(&#39;w2&#39;)
ax.set_xlim((w1_min, w1_max))
ax.set_ylim((w2_min, w2_max))
ax.legend(bbox_to_anchor=(1.2, 1.))
ax.set_title(&amp;quot;Beale Function&amp;quot;)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;output&#34;&gt;Output&lt;/h6&gt;
&lt;p&gt;As seen in the image, blue region indicates lower values and the red region is higher values of the Beale&amp;rsquo;s function. The minima (3, 0.5) is indicated with a star.&lt;/p&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/beale_func.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/beale_func.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h6 id=&#34;setting-up-parameters&#34;&gt;Setting up Parameters&lt;/h6&gt;
&lt;p&gt;As we are using PyTorch, we need to have parameters that are to be optimized put into a &lt;code&gt;nn.Module&lt;/code&gt; class. The &lt;code&gt;__init__()&lt;/code&gt; takes (x, y) as inputs to initialize the parameters (w1, w2).  Also, we are going to write the beale&amp;rsquo;s equation in the forward function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Net_Beale(torch.nn.Module):
    def __init__(self, x, y):
        super(Net_Beale, self).__init__()
        self.w1 = torch.nn.Parameter(torch.tensor([x]))
        self.w2 = torch.nn.Parameter(torch.tensor([y]))

    def forward(self):
        # Beale Function Equation
        a = (1.5 - self.w1 + self.w1*self.w2)**2
        b = (2.25 - self.w1 + self.w1*self.w2**2)**2
        c = (2.625 - self.w1 + self.w1*self.w2**3)**2
        return a+b+c
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;optimizing-and-saving-trajectory&#34;&gt;Optimizing and Saving Trajectory&lt;/h6&gt;
&lt;p&gt;Since we are interested in tracking the path of the optimization, we need to collect the parameters at each step/desired steps and save them for plotting.&lt;/p&gt;
&lt;p&gt;The below function initialises the parameters of the network, initializes an optimizer and runs the optimization for the specified number of steps while collecting the path of the parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_trajectory(x, y, optim, lr, epochs, interval=1):

    # Initialize Network
    net = Net_Beale(x,y)

    # Initialize Optimizer
    if optim == &amp;quot;sgd&amp;quot;:
        optim = torch.optim.SGD(net.parameters(), lr)
    elif optim == &amp;quot;mom&amp;quot;:
        optim = torch.optim.SGD(net.parameters(), lr, momentum=0.9)
    elif optim == &amp;quot;adam&amp;quot;:
        optim = torch.optim.Adam(net.parameters(), lr)

    # Initialize Trackers
    w_1s = []
    w_2s = []

    # Run Optimization
    for i in range(epochs):
        optim.zero_grad()
        o = net()
        o.backward()

        if i % interval == 0:
            # Append current w1 and w2 to trackers
            w_1s.append(net.w1.item())
            w_2s.append(net.w2.item())
        optim.step()

    w_1s.append(net.w1.item())
    w_2s.append(net.w2.item())

    # Join w1&#39;s and w2&#39;s into one array
    trajectory = np.array([w_1s, w_2s])   

    return trajectory
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;comparison-between-trajectories&#34;&gt;Comparison between trajectories&lt;/h6&gt;
&lt;p&gt;After collecting the paths of parameters with different algorithms, we are going to plot them on the Beale Function Contour plot. The below function takes in the initial position, list of optimizers and corresponding learning rates and epochs and plots the trajectories of algorithms with specified settings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compare_trajectories(x, y, epochs, optims, lrs):

    colors = [&#39;k&#39;, &#39;g&#39;, &#39;b&#39;, &#39;r&#39;, &#39;y&#39;, &#39;c&#39;, &#39;m&#39;]
    trajectories = []
    names = []
    # Loop on all optimizers in list
    for ep, optim, lr in zip(epochs, optims, lrs):
        trajectory = get_trajectory(float(x), float(y), optim=optim, lr=lr, epochs=ep)
        names.append(optim)
        trajectories.append(trajectory)

    # Plot the Contour plot of Beale Function and trajectories of optimizers
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.contour(w1, w2, losses, levels=np.logspace(0, 5, 35),
                        norm=LogNorm(), cmap=plt.cm.jet, alpha = 0.5)

    for i, trajectory in enumerate(trajectories):
        ax.quiver(trajectory[0,:-1], trajectory[1,:-1], trajectory[0,1:]-trajectory[0,:-1],
                  trajectory[1,1:]-trajectory[1,:-1], scale_units=&#39;xy&#39;, angles=&#39;xy&#39;, scale=1,
                  color=colors[i], label=names[i], alpha=0.8)

    start_ =[x,y]
    ax.plot(*start_, &#39;r*&#39;, color=&#39;k&#39;,markersize=10, alpha=0.7, label=&#39;start&#39;)
    ax.plot(*minima_, &#39;r*&#39;, color=&#39;r&#39;,markersize=10, alpha=0.7, label=&#39;minima&#39;)
    ax.set_xlabel(&#39;w1&#39;)
    ax.set_ylabel(&#39;w2&#39;)
    ax.set_xlim((w1_min, w1_max))
    ax.set_ylim((w2_min, w2_max))
    ax.set_title(&amp;quot;Initial point - ({},{})&amp;quot;.format(x,y))
    ax.legend(bbox_to_anchor=(1.2, 1.))
    fig.suptitle(&amp;quot;Optimization Trajectory&amp;quot;)
    fig.tight_layout(rect=[0, 0.03, 1, 0.95])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;trying-different-initial-points&#34;&gt;Trying Different Initial Points&lt;/h3&gt;
&lt;p&gt;After setting up everything, we are now ready to compare the three algorithms with different initial points.&lt;/p&gt;
&lt;p&gt;The learning rates observed to be working in this problem&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SGD      - 0.0001&lt;/li&gt;
&lt;li&gt;momentum - 0.0001&lt;/li&gt;
&lt;li&gt;Adam     - 0.01&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are going to use the same learning rate for different initial points considered for respective algorithms to keep the analysis simple and since we are not doing hyperparameter tuning. Feel free to download this 
&lt;a href=&#34;https://github.com/pranaymodukuru/OptimizationTrajectoryPlots/blob/master/OptimizationTrajectory_Initialization.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt; and trying out different hyperparameters and initial points.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Settings for optimizers
epochs = [10000] * 3
optims = [&#39;sgd&#39;, &#39;mom&#39;, &#39;adam&#39;]
lrs = [0.0001, 0.0001, 0.01]
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;case-1--a-point-close-to-minima&#34;&gt;Case 1 : A point close to minima&lt;/h6&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#  A point closer to minima
x = 2.5
y = 2.
compare_trajectories(x, y, epochs, optims, lrs)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/close_to_minima.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/close_to_minima.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;All the three reach the global minima, lets move a little further and see what happens.&lt;/p&gt;
&lt;h6 id=&#34;case-2--moving-a-little-further&#34;&gt;Case 2 : Moving a little further&lt;/h6&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# A little away in the same region
x = 1.5
y = 2.5
compare_trajectories(x, y, epochs, optims, lrs)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/a_little_further.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/a_little_further.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Moving a little further from where we have started has made a huge difference on how an optimizer moves the parameters towards minimum. As seen in the figure above &lt;strong&gt;adam&lt;/strong&gt; optimizer moves towards a local minimum and is stuck there, whereas &lt;strong&gt;sgd&lt;/strong&gt; and &lt;strong&gt;momentum&lt;/strong&gt; reach the global minimum. Things to notice, we are not changing the learning rate here, as we are focusing on effect of initialization on optimization.&lt;/p&gt;
&lt;h6 id=&#34;case-3--moving-a-little-further&#34;&gt;Case 3 : Moving a little further&lt;/h6&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Lower left region
x = -4
y = -4
compare_trajectories(x, y, epochs, optims, lrs)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/lower_left.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/optimization-trajectory-visualization/lower_left.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;(more example can be found in this 
&lt;a href=&#34;https://github.com/pranaymodukuru/OptimizationTrajectoryPlots/blob/master/OptimizationTrajectory_Initialization.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The initial point plays a crucial role in optimization problems. Here we are trying to solve a two dimensional problem which is fairly easy when compared to finding a minima when we have a large dataset and more than million parameters (dimensions).&lt;/p&gt;
&lt;p&gt;Although we are not tuning the hyperparameters here, we can effectively drive the optimization in right direction with the right set of hyperparameters.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://mitpress.mit.edu/books/optimization-machine-learning&#34;&gt;https://mitpress.mit.edu/books/optimization-machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Test_functions_for_optimization&#34;&gt;https://en.wikipedia.org/wiki/Test_functions_for_optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://benchmarkfcns.xyz/benchmarkfcns/bealefcn.html&#34;&gt;http://benchmarkfcns.xyz/benchmarkfcns/bealefcn.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ruder.io/optimizing-gradient-descent/&#34;&gt;https://ruder.io/optimizing-gradient-descent/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/&#34;&gt;http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://communities.sas.com/t5/SAS-Communities-Library/Mathematical-Optimization-in-our-Daily-Lives/ta-p/504724#&#34;&gt;https://communities.sas.com/t5/SAS-Communities-Library/Mathematical-Optimization-in-our-Daily-Lives/ta-p/504724#&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Denoising Autoencoder</title>
      <link>https://pranaymodukuru.github.io/project/denoising-auto-encoder/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://pranaymodukuru.github.io/project/denoising-auto-encoder/</guid>
      <description>&lt;p&gt;Deep Learning has transformed many domains such as Image Processing, Computer Vision, Natural Language Processing. Removing noise from images has been a reasonably tough task until the deep learning based auto encoders transformed the image processing field.&lt;/p&gt;
&lt;p&gt;I used a Deep Convolutional Autoencoder to remove coffe stains, footprints, marks resulting from folding or wrinkles from scanned office documents.&lt;/p&gt;
&lt;p&gt;Check out the project on 
&lt;a href=&#34;https://github.com/pranaymodukuru/DenoisingAutoencoder/blob/master/DenoisingAutoEncoder_NoisyOfficeData.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hand Gesture Recognition</title>
      <link>https://pranaymodukuru.github.io/project/hand-gesture-recognition/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://pranaymodukuru.github.io/project/hand-gesture-recognition/</guid>
      <description>&lt;p&gt;Gesture Recognition is an important application in many domains.
For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In games, to enable the player to control game elements with hand&lt;/li&gt;
&lt;li&gt;In cars, for touch-less dashboards and enhanced safety&lt;/li&gt;
&lt;li&gt;In Language, identifying sign languages&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this project, I collected hand gesture images from my laptop&amp;rsquo;s webcam and trained a Convolutional Neural Network (CNN) to recognize these gestures.&lt;/p&gt;
&lt;p&gt;My future plan is to use the trained CNN in a game, to predict the hand gestures in real-time.&lt;/p&gt;
&lt;p&gt;Check out the project on 
&lt;a href=&#34;https://github.com/pranaymodukuru/Hand-gesture-detection/blob/master/HandGestureRecognitionCNN.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PMSM Rotor Temperature Prediction</title>
      <link>https://pranaymodukuru.github.io/project/pmsm-rotor-temperature/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://pranaymodukuru.github.io/project/pmsm-rotor-temperature/</guid>
      <description>&lt;h3 id=&#34;predicting-rotor-temperature-of-a-permanent-magnet-synchronous-motorpmsm-using-a-convolutional-neural-networkcnn&#34;&gt;Predicting Rotor Temperature of a Permanent Magnet Synchronous Motor(PMSM) using a Convolutional Neural Network(CNN)&lt;/h3&gt;
&lt;h3 id=&#34;1-problem-statement&#34;&gt;1. Problem Statement&lt;/h3&gt;
&lt;p&gt;The rotor temperature of any motor is difficult to measure as it is a rotating part. Placing any sensors to measure this difficult to measure temperature would result in increase in costs and also increase the weight of the motor. In the era of electric vehicles, electric drives have become common in automotives and a lot of research is ongoing to reduce the weight of the motors in order to increase the efficiency of electric cars.&lt;/p&gt;
&lt;p&gt;Measurement of quantities like temperature, torque of the rotor is important in order to design control systems to effectively control the motor. Many statistical based approaches have been studied in estimating the values of temperatures and torque, but these approaches require domain knowledge and often are different for different motors and different operating conditions. There is no universal approach towards estimating these values.&lt;/p&gt;
&lt;p&gt;With the advent of Deep Learning, methods have been proposed to use deep learning approaches to predict the sensor values. The goal of the project is to efficiently predict the rotor temperature of a permanent magnet synchronous motor (PMSM), as it is usually difficult to measure the rotor temperature. This kind of prediction helps to reduce the amount of equipment that is to be mounted on to the motor to measure the temperature.&lt;/p&gt;
&lt;p&gt;Please refer 
&lt;a href=&#34;https://github.com/pranaymodukuru/PMSM_Rotor_Temp_Prediction/blob/master/CNN_MotorTemperature_Regression.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;CNN_MotorTemperature_Regression.ipynb&lt;/em&gt;&lt;/a&gt; for code.&lt;/p&gt;
&lt;h3 id=&#34;2-data-description&#34;&gt;2. Data Description&lt;/h3&gt;
&lt;p&gt;This project uses the electric-motor-temperature dataset to predict the rotor temperature of a motor. The dataset and its description is available here: &lt;a href=&#34;https://www.kaggle.com/wkirgsn/electric-motor-temperature&#34;&gt;https://www.kaggle.com/wkirgsn/electric-motor-temperature&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A brief description of data attributes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ambient&lt;/strong&gt; - Ambient temperature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coolant&lt;/strong&gt; - Coolant temperature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;u_d&lt;/strong&gt; - Voltage d-component (Active component)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;u_q&lt;/strong&gt; - Voltage q-component (Reactive component)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Motor_speed&lt;/strong&gt; - Speed of the motor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Torque&lt;/strong&gt; - Torque induced by current&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;i_d&lt;/strong&gt; - Current d-component (Active component)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;i_q&lt;/strong&gt; - Current q-component (Reactive component)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pm&lt;/strong&gt; - Permanent Magnet Surface temperature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stator_yoke&lt;/strong&gt; - Stator yoke temperature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stator_tooth&lt;/strong&gt; - Stator tooth temperature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stator_winding&lt;/strong&gt; - Stator winding temperature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Profile_id&lt;/strong&gt; - Measurement Session id&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Target features are &lt;strong&gt;pm&lt;/strong&gt;, &lt;strong&gt;Torque&lt;/strong&gt;, and **Stator_*** temperatures.&lt;/p&gt;
&lt;h3 id=&#34;3-data-preparation&#34;&gt;3. Data Preparation&lt;/h3&gt;
&lt;p&gt;The data has been divided into sequences of certain length. For example, A series of values collected from all the sensors over a time period of 10 seconds is taken as a sequence and the target value for this sequence is the temperature value of the next instance.&lt;/p&gt;
&lt;p&gt;This way we can give the raw sensor values to the model and predict the temperature values. Although there are four interesting targets here. Only one of them is used as a target variable in this project.&lt;/p&gt;
&lt;h3 id=&#34;4-modelling-and-evaluation&#34;&gt;4. Modelling and Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Algorithm used - 1-D Convolutional Neural Network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The sequence of sensor values are given as an input to the CNN and the output of CNN is taken as temperature.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation Metric - RMSE&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the target variable is a continuous variable, regression evaluation metric RMSE (Root Mean Squared Error) and R2 Score (Coefficient of Determination) have been used.&lt;/p&gt;
&lt;h3 id=&#34;5-results&#34;&gt;5. Results&lt;/h3&gt;
&lt;h5 id=&#34;loss-vs-epoch&#34;&gt;Loss vs Epoch&lt;/h5&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/pmsm-rotor-temperature/losses.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/pmsm-rotor-temperature/losses.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h4 id=&#34;predictions&#34;&gt;Predictions&lt;/h4&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://pranaymodukuru.github.io/img/pmsm-rotor-temperature/predictions.png&#34; &gt;


  &lt;img src=&#34;https://pranaymodukuru.github.io/img/pmsm-rotor-temperature/predictions.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;6-references&#34;&gt;6. References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/wkirgsn/electric-motor-temperature&#34;&gt;https://www.kaggle.com/wkirgsn/electric-motor-temperature&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
