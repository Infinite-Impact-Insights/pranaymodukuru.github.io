[{"authors":["admin"],"categories":null,"content":"I am an Electrical and Electronics Graduate currently doing my Master\u0026rsquo;s in Systems Engineering and Engineering Management at Fachhochschule Südwestfalen, with a special focus on applications of Machine Learning in Industrial Automation.\nMy Master Thesis is focussed on developing a novel Regularization Algorithm for Multi-Task Lifelong Learning in Deep Neural Networks. Additionally, I am working as a Research Assistant and have a vast experience in devising algorithms for Condition Monitoring, Predictive Maintenance and Computer Vision.\nI am proficient in Deep Learning, Machine Learning, Data Analysis and Visualization. I am passionate about research involving Deep Learning, Computer Vision and Artificial Intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pranaymodukuru.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an Electrical and Electronics Graduate currently doing my Master\u0026rsquo;s in Systems Engineering and Engineering Management at Fachhochschule Südwestfalen, with a special focus on applications of Machine Learning in Industrial Automation.\nMy Master Thesis is focussed on developing a novel Regularization Algorithm for Multi-Task Lifelong Learning in Deep Neural Networks. Additionally, I am working as a Research Assistant and have a vast experience in devising algorithms for Condition Monitoring, Predictive Maintenance and Computer Vision.","tags":null,"title":"Naga Sai Pranay Modukuru","type":"authors"},{"authors":["Pranay Modukuru"],"categories":[],"content":"Image Source\nIntroduction This blog post is about the final project that I did in Udacity\u0026rsquo;s Machine Learning Engineer Nanodegree program. This project is based on real-world data provided by Arvato Financial Solutions. The task is to understand the customer segments of a mail order company which sells organic products, and compare these segments with the general population data to predict future probable customers.\nThis project is as close as it can get to real-world data science project. It was challenging and fun to do and I learnt a lot by working on this and decided to write a blog post about my learnings.\nStructure of this article:\n What is Customer Segmentation Project Introduction Data Description and Analysis Customer Segmentation using Unsupervised Learning Predicting future customers using Supervised Learning  Customer Segmentation Customer segmentation is the process of dividing customers into groups of individuals who share common characteristics so that companies can target each group with tailored marketing strategies. This enables marketers to create targeted marketing messages for specific group of customers which will increase the chances of the person buying a product. It will allow them to create and use specific communication channels to communicate with different segments to attract them. A simple example would be that the companies try to attract younger generation through social media posts and older generation with may be radio advertising. This will help the companies in establishing better customer relationships and their overall performance as an organization.\nThree most common types of Customer Segmentation Although there are more than three types of customer segmentation, we are going to look at the three most common strategies to do customer segmentation\n1. Demographic Segmentation The parameters such as age, gender, education, income, financial status, etc. come under demographics of a person. This kind of segmentation is the most common approach to segment customers, since this data is easy to obtain and analyse. Also, the demographics correspond to the most important characteristics of a person which will help the marketers in making informed decisions. For example an airlines company can send emails about offers on economy class tickets to people coming under low income group, and about first class tickets to high income groups.\n2. Geographic Segmentation As the name suggests, this kind of customer segmentation is done based on the physical location of a person. An example in this case would be a company which manufactures air conditioning systems. It cannot offer same products to people in in India and Iceland.\n3. Behavioral Segmentation This kind of customer segmentation is based on the behavioral data of the customers. The grouping is done based on the purchasing habits, spending habits, brand interactions, browsing history or any other data which corresponds to behaviour or a person. All the targeted ads we see online today uses some kind of behavioral segmentation to decide which ad to target to which customer.\nProject Introduction The data which Arvato has provided in this project is the demographic data of their customers and the demographic data of the general population in Germany. So the task is to do customer segmentation based on the demographic data. The data includes 366 features corresponding to each person, which indicate age, gender, life stage, financial status, family status, family situation, house location, neighbourhood information. These features are only a few of 366 features which are there in the data.\nProblem Statement The problem statement can be formulated as \u0026ldquo;Given the demographic data of a person, how can a mail order company acquire new customers in an efficient way\u0026rdquo;.\nGiven this statement we can conclude that we have to compare the existing customer data and the general population data in someway to deduce a relationship between them. A manual way of doing this is to compare the statistics between the customers and general population. For example, the mean and standard deviation of age can be compared to determine which age group is more likely to be a customer or the salaries can be compared to see what group of people fall into customers, etc.\nBut this analysis would give out many results which again have to be analysed to come up with a final strategy. This process will require a lot of time and by the time this analysis is done, the competitor in the market will capture most of the population and the company will be out of business. Today with the advent of Machine Learning (ML) techniques used in every domain, this problem can also be addressed with the help of ML algorithms.\nData Description and Analysis As explained earlier the data that Arvato provided contains demographics of existing customers and general population data. Additionally two extra files have been provided for supervised learning section, one for training and one for testing. At the end the predictions on the test set were to be submitted to Kaggle competition. Also, two additional files were provided which contain the information about feature values and their description. These two files are very helpful as all the feature names were in German and in short forms. Lets look at the information regarding the dataset.\n General population - consists demographic data for general population in Germany corresponding to 891,211 people with each person having 366 features. (891211x366) Customers Data - consists demographic data for existing customers for the mail-order company corresponding to 191,652 people each with 369 features. The three extra features were company specific regarding how did the place the order and how much quantity the order was. (191652x366) Training data - consists of demographic data of 42,982 people with an additional column other than 366 indicating whether a person is customer or not, to be used to train supervised learning models. Test data - consists demographic data of 42,833 people with the same 366 features but no targets. Two additional files containing information about features  Data Cleaning The data analysis started with replacing the mis-recorded values to NaN values. These mis-recorded values were determined using the information files given. For example, as per the description given in the Attribute information file the column \u0026lsquo;LP_STATS_FEIN\u0026rsquo; needs to contain only values from \u0026lsquo;1-5\u0026rsquo;, but the data which is given contains \u0026lsquo;0\u0026rsquo;. This means that there was some error in recording these values and these values have to be treated as missing values. The attribute information file also contains information about what value corresponds to unknown values in some columns. This information was helpful in a way that all the mis-represented information can be converted to missing values.\n\rColumns with more than 30% missing values\r\r\rAfter cleaning the mis-recorded values the next step is to deal with the missing values itself. An analysis of percentage missing values per column is performed to determine how many columns have missing values and if they did contain how much percentage. Figure 1 shows the columns which have more than 30% missing values. After this analysis a threshold of 30 was selected to drop the columns. Later an analysis on missing values row wise had to be performed in order to remove observations that have missing features. Here a threshold of 50 missing features per observation was selected to drop rows. After this analysis the resulting shapes were:\n General population - (737288x356) Customers data - (13426x356) (neglecting the additional customer specific features)  Feature Engineering There were some categorical features which were encoded with numeric values (in fact many, but only a few were addressed for simplicity). These features were coded with binary encoding. Also, some features were containing too much information, for example information about financial status and age in one single column. Such kind of features were identified and were either recoded to contain broader information or divided into two columns to contain both features separately. Any feature that contained more than 20 categories was either dropped or reconstructed into something useful with the help of the Attribute information file.\nSince many of the features contained categorical features dumped into single column, this step has helped simplifying the data for the later steps. This step resulted in having 353 features.\nImputing Missing Values Even after dropping columns and rows based on certain threshold, we are left with data which still has missing values. This problem is addressed with the help of simple imputer, which fills in the missing data with some values which we can control. A general approach for numerical features would be to impute the missing values with median or mean. But a more common approach for categorical features is to impute with most common values. Since data corresponds to population, so it is more sense to impute the missing values with most common values.\nNow, that the data has been cleaned and ready for modelling, one final step is to scale the data i.e. to bring all the features to the same range. This is done with the help of a standard scaler.\nCustomer Segmentation using Unsupervised Learning For cluster segmentation there are two steps to be performed.\n Dimensionality Reduction Clustering  Dimensionality Reduction Although we have 353 features, not all of them will have variation i.e. some features might be same for all the people. We can go through all the features here to see how many unique values are there per feature to select the features which have the required variation. But a more systematic approach would be to perform a certain analysis before dropping any column. Hence, Principal Component Analysis (PCA) has been performed to analyse the explained variance of the PCA components. PCA performs a linear transformation on the data to form a new coordinate system such that the components in the new coordinate system represent the variation in the data.\n\rPCA Explained Variance plot\r\r\rThis will help us determine how many features have enough variance to explain the variation in the data, and reduce the number of dimensions of the data. An explained variance plot is used to select the number of components i.e. the number of dimensions in the reduced coordinate space. As seen from the above plot almost 90% of the variance can be explained with the help of approximately 150 components. Now, after the PCA transformation we are left with 150 PCA components each made up of a linear combination between the main features.\nClustering After the dimensionality reduction, the next step is to divide the general population and customer population into different segments. K-Means clustering algorithm has been chosen for this task. Since it is simple and is apt for this task, since it measures the distance between two observations to assign a cluster. This algorithm will help us in separating the general population with the help of the reduced features into a specified number of clusters and use this cluster information to understand the similarities in the general population and customer data. The number of clusters is selected to be \u0026lsquo;8\u0026rsquo; with the help of an elbow plot.\nCluster Analysis \rCluster proportions\r\r\rThe general population and the customer population have been clustered into segments. Figure 3 represents the proportions of population coming into each cluster. The cluster distribution of the general population is uniform, meaning that the general population has been uniformly clustered into 8 segments. But the customer population seems to be coming from the clusters ‘0’, ‘3’, ‘4’ and ‘7’. We can further confirm this by taking the ratio of proportions of customers segments and general population segments as shown in Figure 4.\n\rCluster proportions\r\r\rAs seen in Figure 4, if the ratio of proportions is greater than 1 that means this cluster has a greater number of customers in the existing population and has a potential to have more future customers. Whereas if the ratio is less than 1 that means these clusters have the least possibility to have future customers.\nA more detailed cluster analysis, which explains each cluster and corresponding components is also performed. It is documented in the jupyter notebook used for this project. I am not explaining it here as this blog post is already too long.\nCustomer Acquisition using Supervised Learning After analysing the general population and customers data understanding which segments to concentrate on. We can further extend this analysis to make use of ML algorithms to take this decision too. Since we already have the customers data and general population data, we can combine them to form training data and train the ML models to make predictions about whether to approach a customer or not.\nIn this case supervised learning is done with the given train and test data. AUROC score has been selected as the evaluation metric since the problem is a highly imbalanced classification. The baseline performance was set with a Logistic Regression model, which was further improved with the help of Tree based ensemble models. The AdaboostClassifier and XGBoostClassifier were the final selected models, whose predictions were submitted to kaggle to attain a position in top 30 percentile (on the date of submission) with only two submissions.\n\rKaggle Leaderboard\r\r\rConclusion The general population and customer population have been compared and segmented using an Unsupervised learning algorithm. We were able to determine which clusters have more customers and which are potential clusters to have probable customers. We have also used supervised learning algorithms to predict a future possible customer based on the demographic data.\nThe resulting analysis has produced good results to put me in the top 30 percentile in the competition Leader board. Another point to note here is that the top score in the Leaderboard is 0.81063 which is not far away from the score that I achieved (0.80027). Which means, there is still a lot of scope for improvement.\nA more comprehensive explanation of each step and the reasons behind choices of algorithms and metrics has been given in the project report and all the steps are documented in this notebook.\nFinally, I would like to thank Udacity and Arvato Financial Solutions for providing this wonderful opportunity to work with real-world data. This helped me gain valuable experience and helped me use and improve my skills.\nReferences  https://www.business2community.com/customer-experience/4-types-of-customer-segmentation-all-marketers-should-know-02120397 https://blog.alexa.com/types-of-market-segmentation/ https://clevertap.com/blog/customer-segmentation-examples-for-better-mobile-marketing/ https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/ https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba  ","date":1586789157,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586789157,"objectID":"0a43c77d8a4c34b4ed680ba3c9225980","permalink":"https://pranaymodukuru.github.io/post/customer-segmentation/","publishdate":"2020-04-13T16:45:57+02:00","relpermalink":"/post/customer-segmentation/","section":"post","summary":"Using Unsupervised Learning to Cluster Customers into segments based on Demographic data and Supervised Learning to predict potential customers","tags":["Machine Learning","Data Science"],"title":"Customer Segmentation and Acquisition using Machine Learning","type":"post"},{"authors":[],"categories":[],"content":"Image Source\nProject Overview In this project, the demographic data of German population and the customer data have been analysed in order to perform Customer Segmentation and Customer Acquisition. Arvato Financial Solutions is a services company that provides financial services, Information Technology (IT) services and Supply Chain Management (SCM) solutions for business customers on a global scale.\nThis project is to help a Mail-Order company to acquire new customers to sell its organic products. The goal of this project is to understand the customer demographics as compared to general population in order to decide whether to approach a person for future products.\nPlease click on links below for more details  \rProject \rCode  ","date":1586775014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586775014,"objectID":"a786b7a2a15c1bd7d416cd5a390881b5","permalink":"https://pranaymodukuru.github.io/project/customer-segmentation-acquisition/","publishdate":"2020-04-13T12:50:14+02:00","relpermalink":"/project/customer-segmentation-acquisition/","section":"project","summary":"Using Unsupervised and Supervised Learning Techniques","tags":["Machine Learning","Data Science"],"title":"Customer Segmentation and Acquisition","type":"project"},{"authors":null,"categories":null,"content":"Photo by Ricardo Gomez Angel on Unsplash\nConcrete Compressive Strength Prediction using Machine Learning Concrete is one of the most important materials in Civil Engineering. Knowing the compressive strength of concrete is very important when constructing a building or a bridge. The Compressive Strength of Concrete is a highly nonlinear function of ingredients used in making it and their characteristics. Thus, using Machine Learning to predict the Strength could be useful in generating a combination of ingredients which result in high Strength.\nPlease click on links below for more details  \rProject \rCode Medium Blog - Also leave some claps to show appreciation!  ","date":1583712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583712000,"objectID":"f5b973531a5210f476680886003660f3","permalink":"https://pranaymodukuru.github.io/project/concrete-compressive-strength/","publishdate":"2020-03-09T00:00:00Z","relpermalink":"/project/concrete-compressive-strength/","section":"project","summary":"Predicting compressive strength of Concrete using Machine Learning.","tags":["Machine Learning","Industry 4.0","Civil Engineering"],"title":"Concrete Compressive Strength Prediction","type":"project"},{"authors":["Pranay Modukuru"],"categories":[],"content":"Photo by Ricardo Gomez Angel on Unsplash\nConcrete Compressive Strength The Compressive Strength of Concrete determines the quality of Concrete. This is generally determined by a standard crushing test on a concrete cylinder. This requires engineers to build small concrete cylinders with different combinations of raw materials and test these cylinders for strength variations with a change in each raw material. The recommended wait time for testing the cylinder is 28 days to ensure correct results. This consumes a lot of time and requires lot of labour to prepare different prototypes and test them. Also, this method is prone to human error and one small mistake can cause the wait time to drastically increase.\nOne way of reducing the wait time and reducing the amount of combinations to try is to make use of digital simulations, where we can provide information to the computer about what we know and the computer tries different combinations to predict the compressive strength. This way we can reduce the amount of combinations we can try physically and reduce the amount of time for experimentation. But, to design such software we have to know the relations between all the raw materials and how one material affects the strength. It is possible to derive mathematical equations and run simulations based on these equations, but we cannot expect the relations to be same in real-world. Also, these tests have been performed for many number of time now and we have enough real-world data that can be used for predictive modelling.\nIn this article, we are going to analyse Concrete Compressive Strength dataset and build Machine Learning models to predict the compressive strength. This notebook containing all the code can be used in parallel.\nDataset Description The dataset consists of 1030 instances with 9 attributes and has no missing values. There are 8 input variables and 1 output variable. Seven input variables represents the amount of a raw material (measured in $kg/m^3$) and one represents Age (in Days). The target variable is Concrete Compressive Strength measured in ($MPa$ - Mega Pascal). We shall explore the data to see how input features are affecting compressive strength.\nExploratory Data Analysis The first step in a Data Science project is to understand the data and gain insights from the data before doing any modelling. This includes checking for any missing values, plotting the features with respect to the target variable, observing the distributions of all the features and so on. Lets import the data and start analysing.\nLets check the correlations between the input features, this will give an idea about how each variable is affecting all other variables. This can be done by calculating Pearson correlations between the features as shown in the code below.\ncorr = data.corr()\rsns.heatmap(corr, annot=True, cmap='Blues')\r \r\rWe can observe a high positive correlation between compressive Strength (CC_Strength) and Cement. this is true because strength concrete indeed increases with an increase in amount of cement used in preparing it. Also, Age and Super Plasticizer are other two factors influencing Compressive strength.\nThere are other strong correlations between the fetures,\n A strong negative correlation between Super Plasticizer and Water. positive correlations between Super Plasticizer and Fly Ash, Fine Aggregate.  These correlations are useful to understand the data in detail, as they give an idea about how a variable is affecting the other. We can further use a pairplot in seaborn to plot pair wise relations between all the features and distributions of features along the diagonal.\nsns.pairplot(data)\r \r\rThe pair plot gives a visual representation of correlations between all the features.\nWe can plot scatter plots between CC_Strength and other features to see more complex relations.\nCC_Strength vs (Cement, Age, Water) sns.scatterplot(y=\u0026quot;CC_Strength\u0026quot;, x=\u0026quot;Cement\u0026quot;, hue=\u0026quot;Water\u0026quot;,\rsize=\u0026quot;Age\u0026quot;, data=data, ax=ax, sizes=(50, 300))\r \r\rThe observations we can make from this plot,\n Compressive strength increases as amount of cement increases, as the dots move up when we move towards right on the x-axis. Compressive strength increases with age (as the size of dots represents the age), this not the case always but can be up to an extent. Cement with less age requires more cement for higher strength, as the smaller dots are moving up when we move towards right on x-axis. The older the cement is the more water it requires, can be confirmed by observing the colour of the dots. Larger dots with dark colour indicate high age and more water. Concrete strength increases when less water is used in preparing it, since the dots on the lower side (y-axis) are darker and the dots on higher end (y-axis) are brighter.  CC Strength vs (Fine aggregate, Super Plasticizer, Fly Ash) sns.scatterplot(y=\u0026quot;CC_Strength\u0026quot;, x=\u0026quot;FineAggregate\u0026quot;, hue=\u0026quot;FlyAsh\u0026quot;, size=\u0026quot;Superplasticizer\u0026quot;,\rdata=data, ax=ax, sizes=(50, 300))\r \r\rObservations,\n Compressive strength decreases Fly ash increases, as more darker dots are concentrated in the region representing low compressive strength. Compressive strength increases with Super plasticizer, since larger the dot the higher they are in the plot.  We can visually understand 2D, 3D and max up to 4D plots (features represented by colour and size) as shown above, we can further use row wise and column wise plotting features by seaborn to do further analysis, but still we lack the ability to track all these correlations by ourselves. For this reason, we can turn to Machine Learning to capture these relations and give better insights into the problem.\nData preprocessing Before we fit machine learning models on the data, we need to split the data into train, test splits. The features can be rescaled to have a mean of zero and a standard deviation of 1 i.e. all the features fall into the same range.\nX = data.iloc[:,:-1] # Features\ry = data.iloc[:,-1] # Target\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\rsc = StandardScaler()\rX_train = sc.fit_transform(X_train)\rX_test = sc.transform(X_test)\r Model Building After preparing the data, we can fit different models on the training data and compare their performance to choose the algorithm with good performance. As this is a regression problem, we can use RMSE (Root Mean Square Error) and $R^2$ score as evaluation metrics.\n1. Linear Regression We will start with Linear Regression, since this is the go-to algorithm for any regression problem. The algorithm tries to form a linear relationship between the input features and the target variable i.e. it fits a straight line given by, $$y = W*X + b = \\sum_{i=1}^{n} w_i * x_i + b$$ Where $w_i$ corresponds to the coefficient of feature $x_i$.\nThe magnitude of these coefficients can be further controlled by using regularization terms to the cost functions. Adding the sum of the magnitudes of the coefficients will result in the coefficients being close to zero, this variation of linear regression is called Lasso Regression. Adding the sum of squares of the coefficients to the cost function will make the coefficients be in the same range and this variation is called Ridge Regression. Both these variations help in reducing the model complexity and therefore reducing the chances of overfitting on the data.\n# Importing models\rfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\r# Linear Regression\rlr = LinearRegression()\r# Lasso Regression\rlasso = Lasso()\r# Ridge Regression\rridge = Ridge()\r# Fitting models on Training data\rlr.fit(X_train, y_train)\rlasso.fit(X_train, y_train)\rridge.fit(X_train, y_train)\r# Making predictions on Test data\ry_pred_lr = lr.predict(X_test)\ry_pred_lasso = lasso.predict(X_test)\ry_pred_ridge = ridge.predict(X_test)\rfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\rprint(\u0026quot;Model\\t\\t\\t RMSE \\t\\t R2\u0026quot;)\rprint(\u0026quot;\u0026quot;\u0026quot;LinearRegression \\t {:.2f} \\t\\t{:.2f}\u0026quot;\u0026quot;\u0026quot;.format(\rnp.sqrt(mean_squared_error(y_test, y_pred_lr)), r2_score(y_test, y_pred_lr)))\rprint(\u0026quot;\u0026quot;\u0026quot;LassoRegression \\t {:.2f} \\t\\t{:.2f}\u0026quot;\u0026quot;\u0026quot;.format(\rnp.sqrt(mean_squared_error(y_test, y_pred_lasso)), r2_score(y_test, y_pred_lasso)))\rprint(\u0026quot;\u0026quot;\u0026quot;RidgeRegression \\t {:.2f} \\t\\t{:.2f}\u0026quot;\u0026quot;\u0026quot;.format(\rnp.sqrt(mean_squared_error(y_test, y_pred_ridge)), r2_score(y_test, y_pred_ridge)))\r Output    Model RMSE R2     LinearRegression 10.29 0.57   LassoRegression 10.68 0.54   RidgeRegression 10.29 0.57    There is not much difference between the performance with these three algorithms, we can plot the coefficients assigned by the three algorithms for the features with the following code.\ncoeff_lr = lr.coef_\rcoeff_lasso = lasso.coef_\rcoeff_ridge = ridge.coef_\rlabels = req_col_names[:-1]\rx = np.arange(len(labels))\rwidth = 0.3\rfig, ax = plt.subplots(figsize=(10,6))\rrects1 = ax.bar(x - 2*(width/2), coeff_lr, width, label='LR')\rrects2 = ax.bar(x, coeff_lasso, width, label='Lasso')\rrects3 = ax.bar(x + 2*(width/2), coeff_ridge, width, label='Ridge')\rax.set_ylabel('Coefficient')\rax.set_xlabel('Features')\rax.set_title('Feature Coefficients')\rax.set_xticks(x)\rax.set_xticklabels(labels, rotation=45)\rax.legend()\rdef autolabel(rects):\r\u0026quot;\u0026quot;\u0026quot;Attach a text label above each bar in *rects*, displaying its height.\u0026quot;\u0026quot;\u0026quot;\rfor rect in rects:\rheight = rect.get_height()\rax.annotate('{:.2f}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height),\rxytext=(0, 3), textcoords=\u0026quot;offset points\u0026quot;, ha='center', va='bottom')\rautolabel(rects1)\rautolabel(rects2)\rautolabel(rects3)\rfig.tight_layout()\rplt.show()\r \r\rAs seen in the figure, Lasso regression pushes the coefficients towards zero and the coefficients with the normal Linear Regression and Ridge Regression are almost the same.\nWe can further see how the predictions are by plotting the true values and predicted values,\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,4))\rax1.scatter(y_pred_lr, y_test, s=20)\rax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rax1.set_ylabel(\u0026quot;True\u0026quot;)\rax1.set_xlabel(\u0026quot;Predicted\u0026quot;)\rax1.set_title(\u0026quot;Linear Regression\u0026quot;)\rax2.scatter(y_pred_lasso, y_test, s=20)\rax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rax2.set_ylabel(\u0026quot;True\u0026quot;)\rax2.set_xlabel(\u0026quot;Predicted\u0026quot;)\rax2.set_title(\u0026quot;Lasso Regression\u0026quot;)\rax3.scatter(y_pred_ridge, y_test, s=20)\rax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rax3.set_ylabel(\u0026quot;True\u0026quot;)\rax3.set_xlabel(\u0026quot;Predicted\u0026quot;)\rax3.set_title(\u0026quot;Ridge Regression\u0026quot;)\rfig.suptitle(\u0026quot;True vs Predicted\u0026quot;)\rfig.tight_layout(rect=[0, 0.03, 1, 0.95])\r \r\rIf the predicted values and the target values are equal, then the points on the scatter plot will lie on the straight line. As we can see here, non of the model predicts the Compressive Strength correctly.\n2. Decision Trees A Decision Tree Algorithm represents the data with a tree like structure, where each node represents a decision taken on a feature. This algorithm would give better performance in this case, since we have a lot of zeros in some of the input features as seen from their distributions in the pair plot above. This would help the decision trees build trees based on some conditions on features which can further improve performance.\nfrom sklearn.tree import DecisionTreeRegressor\rdtr = DecisionTreeRegressor()\rdtr.fit(X_train, y_train)\ry_pred_dtr = dtr.predict(X_test)\rprint(\u0026quot;Model\\t\\t\\t\\t RMSE \\t\\t R2\u0026quot;)\rprint(\u0026quot;\u0026quot;\u0026quot;Decision Tree Regressor \\t {:.2f} \\t\\t{:.2f}\u0026quot;\u0026quot;\u0026quot;.format(\rnp.sqrt(mean_squared_error(y_test, y_pred_dtr)), r2_score(y_test, y_pred_dtr)))\rplt.scatter(y_test, y_pred_dtr)\rplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rplt.xlabel(\u0026quot;Predicted\u0026quot;)\rplt.ylabel(\u0026quot;True\u0026quot;)\rplt.title(\u0026quot;Decision Tree Regressor\u0026quot;)\rplt.show()\r    Model RMSE R2     Decision Tree Regressor 7.31 0.78    \r\rThe Root Mean Squared Error (RMSE) has come down from 10.29 to 7.31, so the Decision Tree Regressor has improved the performance by a significant amount. This can be observed in the plot as well as more points are closer to the line.\n3. Random Forests Since Using a Decision Tree Regressor has improved our performance, we can further improve the performance by ensembling more trees. Random Forest Regressor trains randomly initialized trees with random subsets of data sampled from the training data, this will make our model more robust.\nfrom sklearn.ensemble import RandomForestRegressor\rrfr = RandomForestRegressor(n_estimators=100)\rrfr.fit(X_train, y_train)\ry_pred_rfr = rfr.predict(X_test)\rprint(\u0026quot;Model\\t\\t\\t\\t RMSE \\t\\t R2\u0026quot;)\rprint(\u0026quot;\u0026quot;\u0026quot;Random Forest Regressor \\t {:.2f} \\t\\t{:.2f}\u0026quot;\u0026quot;\u0026quot;.format(\rnp.sqrt(mean_squared_error(y_test, y_pred_rfr)), r2_score(y_test, y_pred_rfr)))\rplt.scatter(y_test, y_pred_rfr)\rplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rplt.xlabel(\u0026quot;Predicted\u0026quot;)\rplt.ylabel(\u0026quot;True\u0026quot;)\rplt.title(\u0026quot;Random Forest Regressor\u0026quot;)\rplt.show()\r    Model RMSE R2     Random Forest Regressor 5.08 0.89    \r\rThe RMSE has further reduced by ensembling multiple trees. We can plot the feature importance\u0026rsquo;s for tree based models. The feature importance\u0026rsquo;s show how important a feature is for a model when making a prediction.\nfeature_dtr = dtr.feature_importances_\rfeature_rfr = rfr.feature_importances_\rlabels = req_col_names[:-1]\rx = np.arange(len(labels))\rwidth = 0.3\rfig, ax = plt.subplots(figsize=(10,6))\rrects1 = ax.bar(x-(width/2), feature_dtr, width, label='Decision Tree')\rrects2 = ax.bar(x+(width/2), feature_rfr, width, label='Random Forest')\rax.set_ylabel('Importance')\rax.set_xlabel('Features')\rax.set_title('Feature Importance')\rax.set_xticks(x)\rax.set_xticklabels(labels, rotation=45)\rax.legend(loc=\u0026quot;upper left\u0026quot;, bbox_to_anchor=(1,1))\rautolabel(rects1)\rautolabel(rects2)\rfig.tight_layout()\rplt.show()\r \r\rCement and Age are treated as the most important features by tree based models. Fly ash, Coarse and Fine aggregates are least important factors when predicting the strength of Concrete.\nComparison Finally, lets compare the results of all the algorithms.\nmodels = [lr, lasso, ridge, dtr, rfr]\rnames = [\u0026quot;Linear Regression\u0026quot;, \u0026quot;Lasso Regression\u0026quot;, \u0026quot;Ridge Regression\u0026quot;,\r\u0026quot;Decision Tree Regressor\u0026quot;, \u0026quot;Random Forest Regressor\u0026quot;]\rrmses = []\rfor model in models:\rrmses.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\rx = np.arange(len(names))\rwidth = 0.3\rfig, ax = plt.subplots(figsize=(10,7))\rrects = ax.bar(x, rmses, width)\rax.set_ylabel('RMSE')\rax.set_xlabel('Models')\rax.set_title('RMSE with Different Algorithms')\rax.set_xticks(x)\rax.set_xticklabels(names, rotation=45)\rautolabel(rects)\rfig.tight_layout()\rplt.show()\r \r\rConclusion We have analysed the Compressive Strength Data and used Machine Learning to predict the Compressive Strength of Concrete. We have used Linear Regression and its variations, Decision Trees and Random Forests to make predictions and compared their performance. Random Forest Regressor has the lowest RMSE and is a good choice for this problem. Also, we can further improve the performance of the algorithm by tuning the hyperparameters by performing a grid search or random search.\nReferences  I-Cheng Yeh, \u0026ldquo;\rModeling of strength of high performance concrete using artificial neural networks,\u0026rdquo; Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998). Ahsanul Kabir, Md Monjurul Hasan, Khasro Miah, \u0026ldquo;\rStrength Prediction Model for Concrete\u0026quot;, ACEE Int. J. on Civil and Environmental Engineering, Vol. 2, No. 1, Aug 2013. https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength  ","date":1583369632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583369632,"objectID":"33be358ec095b0a6967d73d82fb03ec1","permalink":"https://pranaymodukuru.github.io/post/concrete-compressive-strength/","publishdate":"2020-03-05T01:53:52+01:00","relpermalink":"/post/concrete-compressive-strength/","section":"post","summary":"The purpose of this post is to demonstrate the use of Machine learning as a tool for Civil Engineering","tags":["Machine Learning","Data Analysis","Data Visualization","Industry 4.0"],"title":"Concrete Compressive Strength Prediction using Machine Learning","type":"post"},{"authors":["Pranay Modukuru"],"categories":[],"content":"The Optimization Problem In simple words, finding a minimum value for a given equation is considered as optimization. This has many applications in real life - finding the fastest path when traveling from one place to other, job shop scheduling, air traffic management etc,. The optimization has been the back bone of machine learning, where the algorithms are expected to extract knowledge from huge volumes of data.\nOptimization plays a major role Neural Networks where there are millions of parameters and the goal is to find the right set of parameters to correctly represent the data. There has been a lot of research in this field and many algorithms have been developed for effective optimization. Even though the performance of the optimizer has improved a lot, there is another problem that the optimization depends upon i.e. the initial point. The trajectory of optimization is largely dependant on the initialisation. This has been studied and numerous initialization techniques have been proposed to effectively exploit the power of optimization algorithms.\nIn this post we are going to see how the initialization can affect the performance of some of the optimization algorithms until day. Although, we are using a two dimensional problem here since it is easy to visualize, the initialization problem becomes more prevalent when there are millions of parameters (Neural Networks).\nThe Task Initialize x, y and use gradient descent algorithms to find the optimal values of x and y such that the value of the Beale function is zero (or as low as possible).\nA brief introduction to optimization algorithms We are going to consider three popular optimization algorithms, since we are more concerned about the initialization these will be sufficient for our analysis.\n Stochastic Gradient Descent - The stochastic gradient descent (SGD) algorithm performs one update at a time computing gradients at each step. Momentum - Overcomes the difficulty of slow updates of stochastic gradient descent by considering the momentum of gradients over a period of time. Adam - Considered to be the most popular optimization algorithm. It takes into consideration the first and second moments i.e. the exponentially decaying average of past gradients and squared gradients.  A more detailed explanation about gradient descent optimization algorithms, please read this post by Sebastian Ruder.\nImporting required libraries We are going to use the autograd functionality of PyTorch for getting the gradients and matplotlib for plotting the trajectories.\nimport torch\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rfrom mpl_toolkits.mplot3d import Axes3D\rfrom matplotlib.colors import LogNorm\rimport warnings\rwarnings.filterwarnings(\u0026quot;ignore\u0026quot;)\r The Beale Function $$ f(x, y) = (1.5 - x + xy)^2 + (2.25 - x + xy^2)^2 +(2.625 - x + xy^3)^2\\tag{1} $$\n Beale function is a multimodal non-convex continuous function defined in two dimensions. It is usually evaluated in the range $(x, y) \\in [-4.5, 4.5]$. The function has only one global minimum at $(x, y) = (3, 0.5)$.  The Beale Function, a two dimentional function is chosen to make visualizations simple.\nVisualizing the Beale Function As the Beale function is a two variable function ranging between -4.5 and 4.5, we can generate a meshgrid using numpy to pass all the possible values of x and y to the function. This enables us to have the output of the beale\u0026rsquo;s function at each possible point, we can use these outputs to visualize the function in a contour plot.\nAs we are relating the optimization problem with neural networks, we will refer to (x, y) as (w1 , w2). Also, when using a neural network we refer to objective function as a loss function and the output of the function as loss. In this case, we refer to the Beale\u0026rsquo;s function as loss function and the outputs as losses\n# Defining function\rf = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\r# Defining the range of w1 and w2, step size\rw1_min, w1_max, w1_step = -4.5, 4.5, .2\rw2_min, w2_max, w2_step = -4.5, 4.5, .2\r# Global minima of the function\rminima_ = [3, 0.5]\r# generating meshgrid\rw1, w2 = np.meshgrid(np.arange(w1_min, w1_max + w1_step, w1_step),\rnp.arange(w2_min, w2_max + w2_step, w2_step))\rlosses = f(w1, w2)\r We will now plot the losses on a contour plot with the following code.\nfig, ax = plt.subplots(figsize=(10, 6))\rax.contour(w1, w2, losses, levels=np.logspace(0, 5, 35),\rnorm=LogNorm(), cmap=plt.cm.jet, alpha = 0.8)\rax.plot(*minima_, 'r*', color='r',\rmarkersize=10, alpha=0.7, label='minima')\rax.set_xlabel('w1')\rax.set_ylabel('w2')\rax.set_xlim((w1_min, w1_max))\rax.set_ylim((w2_min, w2_max))\rax.legend(bbox_to_anchor=(1.2, 1.))\rax.set_title(\u0026quot;Beale Function\u0026quot;)\rfig.tight_layout(rect=[0, 0.03, 1, 0.95])\r Output As seen in the image, blue region indicates lower values and the red region is higher values of the Beale\u0026rsquo;s function. The minima (3, 0.5) is indicated with a star.\n\r\rSetting up Parameters As we are using PyTorch, we need to have parameters that are to be optimized put into a nn.Module class. The __init__() takes (x, y) as inputs to initialize the parameters (w1, w2). Also, we are going to write the beale\u0026rsquo;s equation in the forward function.\nclass Net_Beale(torch.nn.Module):\rdef __init__(self, x, y):\rsuper(Net_Beale, self).__init__()\rself.w1 = torch.nn.Parameter(torch.tensor([x]))\rself.w2 = torch.nn.Parameter(torch.tensor([y]))\rdef forward(self):\r# Beale Function Equation\ra = (1.5 - self.w1 + self.w1*self.w2)**2\rb = (2.25 - self.w1 + self.w1*self.w2**2)**2\rc = (2.625 - self.w1 + self.w1*self.w2**3)**2\rreturn a+b+c\r Optimizing and Saving Trajectory Since we are interested in tracking the path of the optimization, we need to collect the parameters at each step/desired steps and save them for plotting.\nThe below function initialises the parameters of the network, initializes an optimizer and runs the optimization for the specified number of steps while collecting the path of the parameters.\ndef get_trajectory(x, y, optim, lr, epochs, interval=1):\r# Initialize Network\rnet = Net_Beale(x,y)\r# Initialize Optimizer\rif optim == \u0026quot;sgd\u0026quot;:\roptim = torch.optim.SGD(net.parameters(), lr)\relif optim == \u0026quot;mom\u0026quot;:\roptim = torch.optim.SGD(net.parameters(), lr, momentum=0.9)\relif optim == \u0026quot;adam\u0026quot;:\roptim = torch.optim.Adam(net.parameters(), lr)\r# Initialize Trackers\rw_1s = []\rw_2s = []\r# Run Optimization\rfor i in range(epochs):\roptim.zero_grad()\ro = net()\ro.backward()\rif i % interval == 0:\r# Append current w1 and w2 to trackers\rw_1s.append(net.w1.item())\rw_2s.append(net.w2.item())\roptim.step()\rw_1s.append(net.w1.item())\rw_2s.append(net.w2.item())\r# Join w1's and w2's into one array\rtrajectory = np.array([w_1s, w_2s]) return trajectory\r Comparison between trajectories After collecting the paths of parameters with different algorithms, we are going to plot them on the Beale Function Contour plot. The below function takes in the initial position, list of optimizers and corresponding learning rates and epochs and plots the trajectories of algorithms with specified settings.\ndef compare_trajectories(x, y, epochs, optims, lrs):\rcolors = ['k', 'g', 'b', 'r', 'y', 'c', 'm']\rtrajectories = []\rnames = []\r# Loop on all optimizers in list\rfor ep, optim, lr in zip(epochs, optims, lrs):\rtrajectory = get_trajectory(float(x), float(y), optim=optim, lr=lr, epochs=ep)\rnames.append(optim)\rtrajectories.append(trajectory)\r# Plot the Contour plot of Beale Function and trajectories of optimizers\rfig, ax = plt.subplots(figsize=(10, 6))\rax.contour(w1, w2, losses, levels=np.logspace(0, 5, 35),\rnorm=LogNorm(), cmap=plt.cm.jet, alpha = 0.5)\rfor i, trajectory in enumerate(trajectories):\rax.quiver(trajectory[0,:-1], trajectory[1,:-1], trajectory[0,1:]-trajectory[0,:-1],\rtrajectory[1,1:]-trajectory[1,:-1], scale_units='xy', angles='xy', scale=1,\rcolor=colors[i], label=names[i], alpha=0.8)\rstart_ =[x,y]\rax.plot(*start_, 'r*', color='k',markersize=10, alpha=0.7, label='start')\rax.plot(*minima_, 'r*', color='r',markersize=10, alpha=0.7, label='minima')\rax.set_xlabel('w1')\rax.set_ylabel('w2')\rax.set_xlim((w1_min, w1_max))\rax.set_ylim((w2_min, w2_max))\rax.set_title(\u0026quot;Initial point - ({},{})\u0026quot;.format(x,y))\rax.legend(bbox_to_anchor=(1.2, 1.))\rfig.suptitle(\u0026quot;Optimization Trajectory\u0026quot;)\rfig.tight_layout(rect=[0, 0.03, 1, 0.95])\r Trying Different Initial Points After setting up everything, we are now ready to compare the three algorithms with different initial points.\nThe learning rates observed to be working in this problem\n SGD - 0.0001 momentum - 0.0001 Adam - 0.01  We are going to use the same learning rate for different initial points considered for respective algorithms to keep the analysis simple and since we are not doing hyperparameter tuning. Feel free to download this notebook and trying out different hyperparameters and initial points.\n# Settings for optimizers\repochs = [10000] * 3\roptims = ['sgd', 'mom', 'adam']\rlrs = [0.0001, 0.0001, 0.01]\r Case 1 : A point close to minima # A point closer to minima\rx = 2.5\ry = 2.\rcompare_trajectories(x, y, epochs, optims, lrs)\r \r\rAll the three reach the global minima, lets move a little further and see what happens.\nCase 2 : Moving a little further # A little away in the same region\rx = 1.5\ry = 2.5\rcompare_trajectories(x, y, epochs, optims, lrs)\r \r\rMoving a little further from where we have started has made a huge difference on how an optimizer moves the parameters towards minimum. As seen in the figure above adam optimizer moves towards a local minimum and is stuck there, whereas sgd and momentum reach the global minimum. Things to notice, we are not changing the learning rate here, as we are focusing on effect of initialization on optimization.\nCase 3 : Moving a little further # Lower left region\rx = -4\ry = -4\rcompare_trajectories(x, y, epochs, optims, lrs)\r \r\r(more example can be found in this notebook)\nConclusion The initial point plays a crucial role in optimization problems. Here we are trying to solve a two dimensional problem which is fairly easy when compared to finding a minima when we have a large dataset and more than million parameters (dimensions).\nAlthough we are not tuning the hyperparameters here, we can effectively drive the optimization in right direction with the right set of hyperparameters.\nReferences  https://mitpress.mit.edu/books/optimization-machine-learning https://en.wikipedia.org/wiki/Test_functions_for_optimization http://benchmarkfcns.xyz/benchmarkfcns/bealefcn.html https://ruder.io/optimizing-gradient-descent/ http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/ https://communities.sas.com/t5/SAS-Communities-Library/Mathematical-Optimization-in-our-Daily-Lives/ta-p/504724#  ","date":1583369632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583369632,"objectID":"d5ad83971469d63a53d5ff519295e9b6","permalink":"https://pranaymodukuru.github.io/post/optimization-trajectory/","publishdate":"2020-03-05T01:53:52+01:00","relpermalink":"/post/optimization-trajectory/","section":"post","summary":"The purpose of this post is to demonstrate the importance of Initialization","tags":["Deep Learning","Optimization","First post"],"title":"Effect of Initialization on Optimization Trajectory in 2D","type":"post"},{"authors":null,"categories":null,"content":"Removing noise from images has been a reasonably tough task until the deep learning based auto encoders transformed the image processing field. I used a Deep Convolutional Autoencoder to remove coffe stains, footprints, marks resulting from folding or wrinkles from scanned office documents.\nPlease click on links below for more details  \rProject \rCode  ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"b1f360d9383396416b3dd170d31c1d9f","permalink":"https://pranaymodukuru.github.io/project/denoising-auto-encoder/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/project/denoising-auto-encoder/","section":"project","summary":"Removing noise/dirt marks from scanned documents using Deep Autoencoder.","tags":["Deep Learning","Machine Learning","Autoencoder","Denoising","Image processing"],"title":"Denoising Autoencoder","type":"project"},{"authors":null,"categories":null,"content":"Gesture Recognition is an important application in many domains. For example:\n In games, to enable the player to control game elements with hand In cars, for touch-less dashboards and enhanced safety In Language, identifying sign languages \u0026hellip;  In this project, I collected hand gesture images from my laptop\u0026rsquo;s webcam and trained a Convolutional Neural Network (CNN) to recognize these gestures.\nMy future plan is to use the trained CNN in a game, to predict the hand gestures in real-time.\nPlease click on links below for more details  \rProject \rCode  ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"58eb9bbf2d45fccfd9b3d93582e8d8c6","permalink":"https://pranaymodukuru.github.io/project/hand-gesture-recognition/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/project/hand-gesture-recognition/","section":"project","summary":"Recognizing Hand Gestures with Convolutional Neural Networks.","tags":["Deep Learning","Machine Learning","Gesture Recognition"],"title":"Hand Gesture Recognition","type":"project"},{"authors":null,"categories":null,"content":"Predicting Rotor Temperature of a Permanent Magnet Synchronous Motor(PMSM) using a Convolutional Neural Network(CNN) The rotor temperature of any motor is difficult to measure as it is a rotating part. Placing any sensors to measure this difficult to measure temperature would result in increase in costs and also increase the weight of the motor. In the era of electric vehicles, electric drives have become common in automotives and a lot of research is ongoing to reduce the weight of the motors in order to increase the efficiency of electric cars.\nMeasurement of quantities like temperature, torque of the rotor is important in order to design control systems to effectively control the motor. Many statistical based approaches have been studied in estimating the values of temperatures and torque, but these approaches require domain knowledge and often are different for different motors and different operating conditions. There is no universal approach towards estimating these values.\nWith the advent of Deep Learning, methods have been proposed to use deep learning approaches to predict the sensor values. The goal of the project is to efficiently predict the rotor temperature of a permanent magnet synchronous motor (PMSM), as it is usually difficult to measure the rotor temperature. This kind of prediction helps to reduce the amount of equipment that is to be mounted on to the motor to measure the temperature.\nPlease click on links below for more details  \rProject \rCode  ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"c408771c433616ff6f4ba074e750692a","permalink":"https://pranaymodukuru.github.io/project/pmsm-rotor-temperature/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/pmsm-rotor-temperature/","section":"project","summary":"Predicting rotor temperature of a Permanent Magnet Synchronous Motor (PMSM) with Convolutional Neural Networks.","tags":["Deep Learning","Machine Learning","Industry 4.0","Soft Sensor"],"title":"PMSM Rotor Temperature Prediction","type":"project"}]